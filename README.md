# Prompt-Learning
Collect recent papers on prompt learning

core thinking
+ When and why CLIP-prior works well?

## Towards vision prompt
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/pdf/2110.04544.pdf) | 2022 | Arxiv | [Link](https://github.com/gaopengcuhk/CLIP-Adapter)|
| [Unsupervised Prompt Learning for Vision-Language Models](https://arxiv.org/pdf/2204.03649.pdf) | 2022 | Arxiv | [Link](https://github.com/tonyhuang2022/UPL)|
| [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | 2022 | CVPR | [Link](https://github.com/KaiyangZhou/CoOp)|
| [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | 2021 | Arxiv | [Link](https://github.com/KaiyangZhou/CoOp)|
| [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf) | 2021 | Arxiv | [Link](https://github.com/OpenAI/CLIP)|
| [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937) | 2017 | Arxiv | [Link](https://github.com/zalandoresearch/pytorch-vq-vae)|

