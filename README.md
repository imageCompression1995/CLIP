# CLIP
Collect recent papers on CLIP and prompt learning

core thinking
+ When and why CLIP-prior works well?
  + CLIP builds connection between vision and language
  + When CLIP is applied to downstream tasks, we typically use prompt-based methods. So, how to learn prompt matters.

## Seminal Work 
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf) | 2021 | ICML | [Link](https://github.com/OpenAI/CLIP)|

## How to transfer CLIP on the classification task
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | 2021 | Arxiv | [Link](https://github.com/KaiyangZhou/CoOp)|
| [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | 2022 | CVPR | [Link](https://github.com/KaiyangZhou/CoOp)|
| [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/pdf/2110.04544.pdf) | 2022 | Arxiv | [Link](https://github.com/gaopengcuhk/CLIP-Adapter)|
| [Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](https://arxiv.org/abs/2111.03930) | 2022 | Arxiv | [Link](https://github.com/gaopengcuhk/Tip-Adapter)|
| [Unsupervised Prompt Learning for Vision-Language Models](https://arxiv.org/pdf/2204.03649.pdf) | 2022 | Arxiv | [Link](https://github.com/tonyhuang2022/UPL)|

## Application

### Detection
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model](https://arxiv.org/pdf/2203.14940.pdf) | 2022 | CVPR | [Link](https://github.com/dyabel/detpro)|

### Image/Video translation/editing
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [HairCLIP: Design Your Hair by Text and Reference Image](https://arxiv.org/abs/2112.05142) | 2022 | CVPR | [Link](https://github.com/wty-ustc/HairCLIP)|
| [FlexIT: Towards Flexible Semantic Image Translation](https://arxiv.org/abs/2203.04705) | 2022 | CVPR| None|
| [VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance](https://arxiv.org/pdf/2204.08583.pdf) | 2022 |Arxiv | [Link](https://github.com/nerdyrodent/VQGAN-CLIP)|

### Image/Video Understanding
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos](https://arxiv.org/pdf/2203.14104.pdf) | 2022 | CVPR | [Link](https://github.com/ttlmh/Bridge-Prompt)|

### Visual Grounding 
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [CPT: Colorful prompt tuning for pre-trained vision-language models](https://openreview.net/pdf?id=TCl7CbQ29hH) | 2021 | Arxiv | None|
| [ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension](https://arxiv.org/pdf/2204.08583.pdf) | 2022 |ACL | None|

