# CLIP
Collect recent papers on CLIP and prompt learning

core thinking
+ Why CLIP-prior works well?
  + CLIP learns relationships between vision and language from 400 million text-image pairs.
+ How to transfer?
  + Zero-shot transfer
  + Prompt learning
  
## Seminal Work 
|  Title   | Year  | Venue | Code |Notes|
|  ----  | ----  | ---- | ---- |---- |
| [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf) | 2021 | ICML | [Link](https://github.com/OpenAI/CLIP)|[Link](https://1drv.ms/u/s!Ahp5ute9Q6jYgTlSy946WgNsk26I)|

## Improved Work 
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm](https://arxiv.org/abs/2110.05208) | 2022 | ICLR | [Link](https://github.com/Sense-GVT/DeCLIP)|
| [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/pdf/2201.12086.pdf) | 2022 | Arxiv | [Link](https://github.com/salesforce/BLIP)|
| [SLIP: Self-supervision meets Language-Image Pre-training](https://arxiv.org/abs/2112.12750) | 2022 | Arxiv | [Link](https://github.com/facebookresearch/SLIP)|


## Applications
### Image Classification
|  Title   | Year  | Venue | Code | 
|  ----  | ----  | ---- | ---- |
| [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134) | 2021 | Arxiv | [Link](https://github.com/KaiyangZhou/CoOp)|
| [Prompt Distribution Learning](https://arxiv.org/pdf/2205.03340.pdf) | 2022 | CVPR | None|
| [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557) | 2022 | CVPR | [Link](https://github.com/KaiyangZhou/CoOp)|
| [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/pdf/2110.04544.pdf) | 2022 | Arxiv | [Link](https://github.com/gaopengcuhk/CLIP-Adapter)|
| [Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](https://arxiv.org/abs/2111.03930) | 2022 | Arxiv | [Link](https://github.com/gaopengcuhk/Tip-Adapter)|
| [Unsupervised Prompt Learning for Vision-Language Models](https://arxiv.org/pdf/2204.03649.pdf) | 2022 | Arxiv | [Link](https://github.com/tonyhuang2022/UPL)|

### Detection
|  Title   | Year  | Venue | Code | Notes|
|  ----  | ----  | ---- | ---- | ---- |
| [Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model](https://arxiv.org/pdf/2203.14940.pdf)| 2022 | CVPR | [Link](https://github.com/dyabel/detpro)|[Link](https://1drv.ms/u/s!Ahp5ute9Q6jYgTlSy946WgNsk26I)|


### Image/Video translation/editing
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [HairCLIP: Design Your Hair by Text and Reference Image](https://arxiv.org/abs/2112.05142) | 2022 | CVPR | [Link](https://github.com/wty-ustc/HairCLIP)|
| [FlexIT: Towards Flexible Semantic Image Translation](https://arxiv.org/abs/2203.04705) | 2022 | CVPR| None|
| [VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance](https://arxiv.org/pdf/2204.08583.pdf) | 2022 |Arxiv | [Link](https://github.com/nerdyrodent/VQGAN-CLIP)|

### Image/Video Understanding
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos](https://arxiv.org/pdf/2203.14104.pdf) | 2022 | CVPR | [Link](https://github.com/ttlmh/Bridge-Prompt)|

### Visual Grounding 
|  Title   | Year  | Venue | Code |
|  ----  | ----  | ---- | ---- |
| [ClipCap: CLIP Prefix for Image Captioning](https://arxiv.org/pdf/2111.09734.pdf) | 2021 | Arxiv | [Link](https://github.com/rmokady/CLIP_prefix_caption)|
| [CPT: Colorful prompt tuning for pre-trained vision-language models](https://openreview.net/pdf?id=TCl7CbQ29hH) | 2021 | Arxiv | None|
| [ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension](https://arxiv.org/pdf/2204.08583.pdf) | 2022 |ACL | None|

